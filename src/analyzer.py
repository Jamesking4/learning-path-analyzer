"""
–ú–æ–¥—É–ª—å –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã—è–≤–ª–µ–Ω–∏—è –∑–∞–∫–æ–Ω–æ–º–µ—Ä–Ω–æ—Å—Ç–µ–π
"""

import pandas as pd
import numpy as np
from scipy import stats
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
import json
from datetime import datetime


class LearningAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –æ–±—Ä–∞–∑–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö"""

    def __init__(self, config):
        self.config = config
        self.scaler = StandardScaler()

    def calculate_basic_metrics(self, df):
        """–†–∞—Å—á–µ—Ç –±–∞–∑–æ–≤—ã—Ö –º–µ—Ç—Ä–∏–∫"""
        metrics = {
            "total_students": df["student_id"].nunique() if not df.empty else 0,
            "total_events": len(df) if not df.empty else 0,
            "avg_events_per_student": (
                len(df) / df["student_id"].nunique()
                if not df.empty and df["student_id"].nunique() > 0
                else 0
            ),
        }

        # –û–±—Ä–∞–±–æ—Ç–∫–∞ time_range –¥–ª—è –ø—É—Å—Ç—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        if (
            not df.empty
            and "event_time" in df.columns
            and df["event_time"].notna().any()
        ):
            metrics["time_range"] = {
                "start": df["event_time"].min().strftime("%Y-%m-%d"),
                "end": df["event_time"].max().strftime("%Y-%m-%d"),
            }
        else:
            metrics["time_range"] = {"start": "N/A", "end": "N/A"}

        if "event_category" in df.columns and not df.empty:
            metrics["event_distribution"] = (
                df["event_category"].value_counts().to_dict()
            )
        else:
            metrics["event_distribution"] = {}

        if "grade" in df.columns and not df.empty:
            grade_data = df[df["grade"] > 0]["grade"]
            if len(grade_data) > 0:
                metrics["grade_stats"] = {
                    "mean": float(grade_data.mean()),
                    "median": float(grade_data.median()),
                    "std": float(grade_data.std()),
                    "min": float(grade_data.min()),
                    "max": float(grade_data.max()),
                }

        return metrics

    def calculate_correlations(self, df):
        """–†–∞—Å—á–µ—Ç –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π –º–µ–∂–¥—É –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å—é –∏ —É—Å–ø–µ–≤–∞–µ–º–æ—Å—Ç—å—é"""
        if "grade" not in df.columns:
            print("‚ö†Ô∏è No grade data available for correlation analysis")
            return pd.DataFrame()

        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö –ø–æ —Å—Ç—É–¥–µ–Ω—Ç–∞–º
        student_data = self._prepare_student_data(df)

        # –í—ã–±–æ—Ä —á–∏—Å–ª–æ–≤—ã—Ö –∫–æ–ª–æ–Ω–æ–∫ –¥–ª—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–∏
        numeric_cols = student_data.select_dtypes(include=[np.number]).columns

        # –†–∞—Å—á–µ—Ç –º–∞—Ç—Ä–∏—Ü—ã –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π
        correlation_matrix = student_data[numeric_cols].corr(method="pearson")

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏–π —Å –æ—Ü–µ–Ω–∫–∞–º–∏
        if "grade_mean" in correlation_matrix.columns:
            grade_correlations = correlation_matrix["grade_mean"].sort_values(
                ascending=False
            )
            print("\nüìä Top correlations with grades:")
            for feature, corr in grade_correlations.head(10).items():
                if feature != "grade_mean" and abs(corr) > 0.1:
                    print(f"   {feature}: {corr:.3f}")

        return correlation_matrix

    def _prepare_student_data(self, df):
        """–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Å—Ç—É–¥–µ–Ω—Ç–æ–≤"""
        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –ø–æ —Å—Ç—É–¥–µ–Ω—Ç–∞–º
        agg_funcs = {"event_time": "count", "grade": ["mean", "max", "min", "std"]}

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∞–≥—Ä–µ–≥–∞—Ü–∏–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Å–æ–±—ã—Ç–∏–π
        if "event_category" in df.columns:
            event_dummies = pd.get_dummies(df["event_category"], prefix="event")
            df = pd.concat([df, event_dummies], axis=1)

            for col in event_dummies.columns:
                agg_funcs[col] = "sum"

        # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        student_data = df.groupby("student_id").agg(agg_funcs)

        # –£–ø—Ä–æ—â–µ–Ω–∏–µ –º—É–ª—å—Ç–∏–∏–Ω–¥–µ–∫—Å–∞
        student_data.columns = [
            "_".join(col).strip() for col in student_data.columns.values
        ]

        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –º–µ—Ç—Ä–∏–∫
        student_data["activity_days"] = df.groupby("student_id")["date"].nunique()

        # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –¥–∞–Ω–Ω—ã—Ö
        numeric_cols = student_data.select_dtypes(include=[np.number]).columns
        student_data[numeric_cols] = self.scaler.fit_transform(
            student_data[numeric_cols]
        )

        return student_data.reset_index()

    def cluster_students(self, df, n_clusters=None):
        """–ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ –ø–æ —Å—Ç–∏–ª—è–º –æ–±—É—á–µ–Ω–∏—è"""
        if n_clusters is None:
            n_clusters = self.config["analysis"]["clustering_n_clusters"]

        # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
        student_data = self._prepare_student_data(df)

        if len(student_data) < n_clusters:
            print(f"‚ö†Ô∏è Not enough students for {n_clusters} clusters")
            return pd.DataFrame()

        # –í—ã–±–æ—Ä –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –¥–ª—è –∫–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏–∏
        feature_cols = [
            col
            for col in student_data.columns
            if col not in ["student_id"]
            and student_data[col].dtype in [np.float64, np.int64]
        ]

        # –ö–ª–∞—Å—Ç–µ—Ä–∏–∑–∞—Ü–∏—è K-Means
        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
        student_data["cluster"] = kmeans.fit_predict(student_data[feature_cols])

        # –ê–Ω–∞–ª–∏–∑ –∫–ª–∞—Å—Ç–µ—Ä–æ–≤
        cluster_stats = student_data.groupby("cluster")[feature_cols].mean()

        print(f"\nüéØ Student clustering results ({n_clusters} clusters):")
        for cluster_id in range(n_clusters):
            cluster_size = (student_data["cluster"] == cluster_id).sum()
            print(f"   Cluster {cluster_id}: {cluster_size} students")

        return student_data[["student_id", "cluster"]]

    def analyze_time_patterns(self, df):
        """–ê–Ω–∞–ª–∏–∑ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏"""
        patterns = {}

        # –ü–æ—á–∞—Å–æ–≤–æ–µ —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ
        df["hour"] = df["event_time"].dt.hour
        hourly_pattern = df.groupby("hour").size()
        patterns["hourly_distribution"] = hourly_pattern.to_dict()

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –¥–Ω—è–º –Ω–µ–¥–µ–ª–∏
        df["day_of_week"] = df["event_time"].dt.dayofweek
        daily_pattern = df.groupby("day_of_week").size()
        patterns["daily_distribution"] = daily_pattern.to_dict()

        # –ê–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –ø–æ —Ç–∏–ø–∞–º —Å–æ–±—ã—Ç–∏–π –≤ —Ç–µ—á–µ–Ω–∏–µ –¥–Ω—è
        if "event_category" in df.columns:
            event_hourly = (
                df.groupby(["hour", "event_category"]).size().unstack(fill_value=0)
            )
            patterns["event_by_hour"] = event_hourly.to_dict()

        return patterns

    def identify_learning_patterns(self, df):
        """–í—ã—è–≤–ª–µ–Ω–∏–µ —É—Å–ø–µ—à–Ω—ã—Ö —É—á–µ–±–Ω—ã—Ö –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤"""
        successful_students = self._identify_successful_students(df)

        if successful_students.empty:
            return {}

        # –ê–Ω–∞–ª–∏–∑ –ø–∞—Ç—Ç–µ—Ä–Ω–æ–≤ —É—Å–ø–µ—à–Ω—ã—Ö —Å—Ç—É–¥–µ–Ω—Ç–æ–≤
        patterns = {}

        # –ü–æ–ª—É—á–∞–µ–º ID —É—Å–ø–µ—à–Ω—ã—Ö —Å—Ç—É–¥–µ–Ω—Ç–æ–≤
        successful_ids = successful_students.index.tolist()

        # –§–∏–ª—å—Ç—Ä—É–µ–º –∏—Å—Ö–æ–¥–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
        successful_df = df[df["student_id"].isin(successful_ids)]

        if successful_df.empty:
            return patterns

        # –ß–∞—Å—Ç–æ—Ç–∞ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–µ–π (–∏—Å–ø–æ–ª—å–∑—É–µ–º count –≤–º–µ—Å—Ç–æ event_time_count)
        patterns["activity_frequency"] = len(successful_df) / len(successful_ids)

        # –†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º —Å–æ–±—ã—Ç–∏–π
        if "event_category" in successful_df.columns:
            event_dist = successful_df["event_category"].value_counts(normalize=True)
            patterns["event_distribution"] = event_dist.to_dict()

        # –í—Ä–µ–º–µ–Ω–Ω—ã–µ –ø–∞—Ç—Ç–µ—Ä–Ω—ã
        if "hour" in successful_df.columns:
            patterns["preferred_hours"] = successful_df["hour"].mode().tolist()

        if "day_of_week" in successful_df.columns:
            patterns["preferred_days"] = successful_df["day_of_week"].mode().tolist()

        # –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏
        if "activity_duration" in successful_df.columns:
            patterns["avg_duration"] = successful_df["activity_duration"].mean()

        return patterns

    def _identify_successful_students(self, df, threshold=None):
        """–ò–¥–µ–Ω—Ç–∏—Ñ–∏–∫–∞—Ü–∏—è —É—Å–ø–µ—à–Ω—ã—Ö —Å—Ç—É–¥–µ–Ω—Ç–æ–≤"""
        if threshold is None:
            threshold = self.config["analysis"]["min_grade_threshold"]

        if "grade" not in df.columns:
            return pd.DataFrame()

        # –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –ø–æ —Å—Ç—É–¥–µ–Ω—Ç—É
        student_grades = (
            df[df["grade"] > 0].groupby("student_id")["grade"].agg(["mean", "count"])
        )

        # –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Å—Ç—É–¥–µ–Ω—Ç–æ–≤ —Å –¥–æ—Å—Ç–∞—Ç–æ—á–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –æ—Ü–µ–Ω–æ–∫
        min_grades = 3  # –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ü–µ–Ω–æ–∫
        valid_students = student_grades[student_grades["count"] >= min_grades]

        # –£—Å–ø–µ—à–Ω—ã–µ —Å—Ç—É–¥–µ–Ω—Ç—ã
        successful = valid_students[valid_students["mean"] >= threshold]

        return successful.reset_index()

    def save_results(self, results, file_path):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        print(f"üíæ Results saved to {file_path}")
